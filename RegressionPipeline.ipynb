{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterBoard(board):\n",
    "    if  \"cbse\" in board.lower():\n",
    "        return \"cbse\"\n",
    "    elif \"central board\" in board.lower():\n",
    "        return \"cbse\"\n",
    "    elif \"icse\" in board.lower():\n",
    "        return \"icse\"\n",
    "    elif board == \"0\":\n",
    "        return \"na\"\n",
    "    else:\n",
    "        return \"state\"\n",
    "\n",
    "def getSpecialisationStats(series):\n",
    "    dict = {}\n",
    "    for index, value in series.iteritems():\n",
    "        list = value.split(\" \")\n",
    "        for str in list:\n",
    "            if str in dict:\n",
    "                dict[str] += 1\n",
    "            else:\n",
    "                dict[str] = 1\n",
    "    return dict\n",
    "\n",
    "def addCustomSpecialisationColumns(dataframe):\n",
    "    series = dataframe['Specialization']\n",
    "    searchwords = [\"computer\",\"mechanical\",\"biotechnology\",\"science\",\"applied\",\"instrumentation\",\"electronics\",\"electrical\",\"application\",\"production\",\"information\",\"eng\",\"telecommunications\",\"technology\",\"civil\",\"industrial\",\"control\",\"engineering\",\"other\"]\n",
    "    customColumns = {\"computer\":[],\"mechanical\":[],\"biotechnology\":[],\"science\":[],\"applied\":[],\"instrumentation\":[],\"electronics\":[],\"electrical\":[],\"application\":[],\"production\":[],\"information\":[],\"eng\":[],\"telecommunications\":[],\"technology\":[],\"civil\":[],\"industrial\":[],\"control\":[],\"engineering\":[], \"other\":[]}\n",
    "    # For each row in the column,\n",
    "    for row in dataframe['Specialization']:\n",
    "        keywords = row.lower().split(\" \")\n",
    "        tempmap = {\"computer\":False,\"mechanical\":False,\"biotechnology\":False,\"science\":False,\"applied\":False,\"instrumentation\":False,\"electronics\":False,\"electrical\":False,\"application\":False,\"production\":False,\"information\":False,\"eng\":False,\"telecommunications\":False,\"technology\":False,\"civil\":False,\"industrial\":False,\"control\":False,\"engineering\":False,\"other\":False}\n",
    "        someKeyWasFound = False\n",
    "        for word in keywords:\n",
    "            if word in searchwords:\n",
    "                if word == \"eng\":\n",
    "                    customColumns[\"engineering\"].append(1)\n",
    "                    tempmap[\"engineering\"] = True\n",
    "                    someKeyWasFound = True\n",
    "                else:\n",
    "                    customColumns[word].append(1)\n",
    "                    tempmap[word] = True\n",
    "                    someKeyWasFound = True\n",
    "\n",
    "        if someKeyWasFound == False:\n",
    "            customColumns[\"other\"].append(1)\n",
    "            tempmap[\"other\"]=True\n",
    "\n",
    "        # print len(tempmap)\n",
    "        for key,value in tempmap.iteritems():\n",
    "            if value is False:\n",
    "                customColumns[key].append(0)\n",
    "\n",
    "\n",
    "    # this should all be of same length\n",
    "    for key, list in customColumns.iteritems():\n",
    "        # print len(list)\n",
    "        if key!=\"eng\":\n",
    "            dataframe[key] = list\n",
    "\n",
    "    return searchwords\n",
    "\n",
    "\n",
    "def modifyScoreColumns(dataframe):\n",
    "    columns = [\"Domain\",\t\"ComputerProgramming\",\t\"ElectronicsAndSemicon\"\t,\"ComputerScience\", \t\"MechanicalEngg\"\t,\"ElectricalEngg\",\t\"TelecomEngg\",\t\"CivilEngg\"]\n",
    "    for i, row in dataframe.iterrows():\n",
    "        for column in columns:\n",
    "            value = row[column]\n",
    "            if value == -1:\n",
    "                dataframe.set_value(i,column,0)\n",
    "    return columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    T = 32\n",
    "    df = pd.read_csv(\"train.csv\", sep=',')\n",
    "    # city = df['JobCity']\n",
    "    # distinctCities = np.sort(city.unique())\n",
    "    # print df['Gender'].unique()\n",
    "    # print df['CollegeTier'].unique()\n",
    "    #print df['Degree'].unique()\n",
    "    # print df['CollegeState'].unique()\n",
    "    # print df['CollegeCityTier'].unique()\n",
    "    # df = df.tail()\n",
    "\n",
    "    dumCompanyName = pd.get_dummies(df['Gender'])\n",
    "    result = dumCompanyName\n",
    "\n",
    "    df['10board'] = df['10board'].apply(filterBoard)\n",
    "    df['12board'] = df['12board'].apply(filterBoard)\n",
    "    dataframe = df['10board']\n",
    "    dummyFrame = pd.get_dummies(dataframe)\n",
    "    result = pd.concat([result, dummyFrame], axis=1)\n",
    "\n",
    "    dataframe = df['12board']\n",
    "    dummyFrame = pd.get_dummies(dataframe)\n",
    "    result = pd.concat([result, dummyFrame], axis=1)\n",
    "\n",
    "    dataframe = df['Degree']\n",
    "    dummyFrame = pd.get_dummies(dataframe)\n",
    "    result = pd.concat([result, dummyFrame], axis=1)\n",
    "\n",
    "    searchwords = addCustomSpecialisationColumns(df)\n",
    "    for word in searchwords:\n",
    "        if word!=\"eng\":\n",
    "            dummyFrame = df[word]\n",
    "            result = pd.concat([result, dummyFrame], axis=1)\n",
    "\n",
    "    dataframe =df['CollegeState']\n",
    "    dummyFrame = pd.get_dummies(dataframe)\n",
    "    result = pd.concat([result, dummyFrame], axis=1)\n",
    "\n",
    "    yearConverter = lambda x: x.year\n",
    "    df['myDOB'] = pd.to_datetime(df['DOB'])\n",
    "    df['myDOB'] = df['myDOB'].apply(yearConverter)\n",
    "    df['12age'] = df['12graduation'] - df['myDOB']\n",
    "    df['gradage'] = df['GraduationYear'] - df['myDOB']\n",
    "\n",
    "    result = pd.concat([result, df['myDOB']], axis=1)\n",
    "    result = pd.concat([result, df['12age']], axis=1)\n",
    "    result = pd.concat([result, df['gradage']], axis=1)\n",
    "    result = pd.concat([result, df['GraduationYear']], axis=1)\n",
    "    result = pd.concat([result, df['12graduation']], axis=1)\n",
    "\n",
    "    result = pd.concat([result, df['10percentage']], axis=1)\n",
    "    result = pd.concat([result, df['12percentage']], axis=1)\n",
    "    result = pd.concat([result, df['CollegeTier']], axis=1)\n",
    "    result = pd.concat([result, df['collegeGPA']], axis=1)\n",
    "    result = pd.concat([result, df['CollegeCityTier']], axis=1)\n",
    "    result = pd.concat([result, df['English']], axis=1)\n",
    "    result = pd.concat([result, df['Logical']], axis=1)\n",
    "    result = pd.concat([result, df['Quant']], axis=1)\n",
    "\n",
    "    columns = modifyScoreColumns(df)\n",
    "    for column in columns:\n",
    "        result = pd.concat([result, df[column]], axis=1)\n",
    "\n",
    "    result = pd.concat([result, df['conscientiousness']], axis=1)\n",
    "    result = pd.concat([result, df['agreeableness']], axis=1)\n",
    "    result = pd.concat([result, df['extraversion']], axis=1)\n",
    "    result = pd.concat([result, df['nueroticism']], axis=1)\n",
    "    result = pd.concat([result, df['openess_to_experience']], axis=1)\n",
    "\n",
    "    result = pd.concat([result, df['Salary']], axis=1)\n",
    "    result.to_csv(\"DesignMatrixHim\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new feature - Salary bins based on 5 - Quantiles of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dm = pd.read_csv(\"DesignMatrixHim\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salary_bins = dm['Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins = pd.qcut(salary_bins, [0, .20, .4, .60, .8, 1.],labels=[1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 4, 1]\n",
       "Categories (5, int64): [1 < 2 < 3 < 4 < 5]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dm['salary_bins'] = bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Unnamed: 0', u'f', u'm', u'cbse', u'icse', u'na', u'state', u'cbse.1',\n",
       "       u'icse.1', u'na.1', u'state.1', u'B.Tech/B.E.', u'M.Sc. (Tech.)',\n",
       "       u'M.Tech./M.E.', u'MCA', u'computer', u'mechanical', u'biotechnology',\n",
       "       u'science', u'applied', u'instrumentation', u'electronics',\n",
       "       u'electrical', u'application', u'production', u'information',\n",
       "       u'telecommunications', u'technology', u'civil', u'industrial',\n",
       "       u'control', u'engineering', u'other', u'Andhra Pradesh', u'Assam',\n",
       "       u'Bihar', u'Chhattisgarh', u'Delhi', u'Goa', u'Gujarat', u'Haryana',\n",
       "       u'Himachal Pradesh', u'Jammu and Kashmir', u'Jharkhand', u'Karnataka',\n",
       "       u'Kerala', u'Madhya Pradesh', u'Maharashtra', u'Meghalaya', u'Orissa',\n",
       "       u'Punjab', u'Rajasthan', u'Sikkim', u'Tamil Nadu', u'Telangana',\n",
       "       u'Union Territory', u'Uttar Pradesh', u'Uttarakhand', u'West Bengal',\n",
       "       u'myDOB', u'12age', u'gradage', u'GraduationYear', u'12graduation',\n",
       "       u'10percentage', u'12percentage', u'CollegeTier', u'collegeGPA',\n",
       "       u'CollegeCityTier', u'English', u'Logical', u'Quant', u'Domain',\n",
       "       u'ComputerProgramming', u'ElectronicsAndSemicon', u'ComputerScience',\n",
       "       u'MechanicalEngg', u'ElectricalEngg', u'TelecomEngg', u'CivilEngg',\n",
       "       u'conscientiousness', u'agreeableness', u'extraversion', u'nueroticism',\n",
       "       u'openess_to_experience', u'Salary', u'salary_bins'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dm['salary_bins']\n",
    "dm.to_csv(\"designMatrixHim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 5]),\n",
       " array([18,  4,  2,  7,  0]),\n",
       " array([1012,  635,  928,  671,  752]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = pd.read_csv(\"designMatrixHim\")\n",
    "bins = dm['salary_bins'].values\n",
    "np.unique(bins, return_index = True, return_counts = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Pipeline and Grid Search for Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputFrame = pd.read_csv(\"DesignMatrix\")\n",
    "# print inputFrame.tail()\n",
    "input = inputFrame.as_matrix()\n",
    "X = input[:,1:-1]\n",
    "Y = input[:,-1:]\n",
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size =500, random_state=5)\n",
    "X_scaled = scale(X)\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_trans = scaler.transform(X)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Ridge class custom score function based on percentage error:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Custom_Ridge(Ridge):\n",
    "    def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,\n",
    "                 copy_X=True, max_iter=None, tol=1e-3, solver=\"auto\",\n",
    "                 random_state=None):\n",
    "        super(Custom_Ridge, self).__init__(alpha=alpha, fit_intercept=fit_intercept,\n",
    "                                    normalize=normalize, copy_X=copy_X,\n",
    "                                    max_iter=max_iter, tol=tol, solver=solver,\n",
    "                                    random_state=random_state)\n",
    "        \n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        y_t = y\n",
    "        y_p = self.predict(X)\n",
    "        r,c = y_t.shape\n",
    "        J = y_p - y_t\n",
    "        loss = np.divide(J,y_t)\n",
    "        meanLoss = np.sum(np.square(loss))/r\n",
    "        return meanLoss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline and gridsearch for Custom Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('preprocessing', StandardScaler()),\n",
    "    ('custom_ridge', Custom_Ridge()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'custom_ridge__alpha': (0.00001, 0.001, 0.001, 0.01, 0.1, 1.0, 10, 100, 1000, 100000 ),\n",
    "    'custom_ridge__max_iter': (10, 100, 1000, 10000),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  49 tasks       | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    }
   ],
   "source": [
    "gr = grid_search.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'custom_ridge__alpha': 1.0, 'custom_ridge__max_iter': 10}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.3267473784919925"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 9.32172, std: 12.27391, params: {'custom_ridge__alpha': 1e-05, 'custom_ridge__max_iter': 10},\n",
       " mean: 9.32172, std: 12.27391, params: {'custom_ridge__alpha': 1e-05, 'custom_ridge__max_iter': 100},\n",
       " mean: 9.32172, std: 12.27391, params: {'custom_ridge__alpha': 1e-05, 'custom_ridge__max_iter': 1000},\n",
       " mean: 9.32172, std: 12.27391, params: {'custom_ridge__alpha': 1e-05, 'custom_ridge__max_iter': 10000},\n",
       " mean: 9.32173, std: 12.27392, params: {'custom_ridge__alpha': 0.001, 'custom_ridge__max_iter': 10},\n",
       " mean: 9.32173, std: 12.27392, params: {'custom_ridge__alpha': 0.001, 'custom_ridge__max_iter': 100},\n",
       " mean: 9.32173, std: 12.27392, params: {'custom_ridge__alpha': 0.001, 'custom_ridge__max_iter': 1000},\n",
       " mean: 9.32173, std: 12.27392, params: {'custom_ridge__alpha': 0.001, 'custom_ridge__max_iter': 10000},\n",
       " mean: 9.32173, std: 12.27392, params: {'custom_ridge__alpha': 0.001, 'custom_ridge__max_iter': 10},\n",
       " mean: 9.32173, std: 12.27392, params: {'custom_ridge__alpha': 0.001, 'custom_ridge__max_iter': 100},\n",
       " mean: 9.32173, std: 12.27392, params: {'custom_ridge__alpha': 0.001, 'custom_ridge__max_iter': 1000},\n",
       " mean: 9.32173, std: 12.27392, params: {'custom_ridge__alpha': 0.001, 'custom_ridge__max_iter': 10000},\n",
       " mean: 9.32179, std: 12.27401, params: {'custom_ridge__alpha': 0.01, 'custom_ridge__max_iter': 10},\n",
       " mean: 9.32179, std: 12.27401, params: {'custom_ridge__alpha': 0.01, 'custom_ridge__max_iter': 100},\n",
       " mean: 9.32179, std: 12.27401, params: {'custom_ridge__alpha': 0.01, 'custom_ridge__max_iter': 1000},\n",
       " mean: 9.32179, std: 12.27401, params: {'custom_ridge__alpha': 0.01, 'custom_ridge__max_iter': 10000},\n",
       " mean: 9.32239, std: 12.27488, params: {'custom_ridge__alpha': 0.1, 'custom_ridge__max_iter': 10},\n",
       " mean: 9.32239, std: 12.27488, params: {'custom_ridge__alpha': 0.1, 'custom_ridge__max_iter': 100},\n",
       " mean: 9.32239, std: 12.27488, params: {'custom_ridge__alpha': 0.1, 'custom_ridge__max_iter': 1000},\n",
       " mean: 9.32239, std: 12.27488, params: {'custom_ridge__alpha': 0.1, 'custom_ridge__max_iter': 10000},\n",
       " mean: 9.32675, std: 12.28115, params: {'custom_ridge__alpha': 1.0, 'custom_ridge__max_iter': 10},\n",
       " mean: 9.32675, std: 12.28115, params: {'custom_ridge__alpha': 1.0, 'custom_ridge__max_iter': 100},\n",
       " mean: 9.32675, std: 12.28115, params: {'custom_ridge__alpha': 1.0, 'custom_ridge__max_iter': 1000},\n",
       " mean: 9.32675, std: 12.28115, params: {'custom_ridge__alpha': 1.0, 'custom_ridge__max_iter': 10000},\n",
       " mean: 9.29090, std: 12.23048, params: {'custom_ridge__alpha': 10, 'custom_ridge__max_iter': 10},\n",
       " mean: 9.29090, std: 12.23048, params: {'custom_ridge__alpha': 10, 'custom_ridge__max_iter': 100},\n",
       " mean: 9.29090, std: 12.23048, params: {'custom_ridge__alpha': 10, 'custom_ridge__max_iter': 1000},\n",
       " mean: 9.29090, std: 12.23048, params: {'custom_ridge__alpha': 10, 'custom_ridge__max_iter': 10000},\n",
       " mean: 8.47100, std: 11.07695, params: {'custom_ridge__alpha': 100, 'custom_ridge__max_iter': 10},\n",
       " mean: 8.47100, std: 11.07695, params: {'custom_ridge__alpha': 100, 'custom_ridge__max_iter': 100},\n",
       " mean: 8.47100, std: 11.07695, params: {'custom_ridge__alpha': 100, 'custom_ridge__max_iter': 1000},\n",
       " mean: 8.47100, std: 11.07695, params: {'custom_ridge__alpha': 100, 'custom_ridge__max_iter': 10000},\n",
       " mean: 4.66078, std: 5.71332, params: {'custom_ridge__alpha': 1000, 'custom_ridge__max_iter': 10},\n",
       " mean: 4.66078, std: 5.71332, params: {'custom_ridge__alpha': 1000, 'custom_ridge__max_iter': 100},\n",
       " mean: 4.66078, std: 5.71332, params: {'custom_ridge__alpha': 1000, 'custom_ridge__max_iter': 1000},\n",
       " mean: 4.66078, std: 5.71332, params: {'custom_ridge__alpha': 1000, 'custom_ridge__max_iter': 10000},\n",
       " mean: 0.81972, std: 0.06166, params: {'custom_ridge__alpha': 100000, 'custom_ridge__max_iter': 10},\n",
       " mean: 0.81972, std: 0.06166, params: {'custom_ridge__alpha': 100000, 'custom_ridge__max_iter': 100},\n",
       " mean: 0.81972, std: 0.06166, params: {'custom_ridge__alpha': 100000, 'custom_ridge__max_iter': 1000},\n",
       " mean: 0.81972, std: 0.06166, params: {'custom_ridge__alpha': 100000, 'custom_ridge__max_iter': 10000}]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing', StandardScaler(copy=True, with_mean=True, with_std=True)), ('custom_ridge', Custom_Ridge(alpha=1e-07, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "       normalize=False, random_state=None, solver='auto', tol=0.001))])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.set_params(preprocessing__with_mean=True,preprocessing__with_std=True,custom_ridge__alpha=0.0000001).fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = pipe.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60919032331739598"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with sklearn Ridge and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17652324704301658"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('preprocessing', StandardScaler()),\n",
    "    ('ridge', Ridge()),\n",
    "])\n",
    "\n",
    "pipe.set_params(preprocessing__with_mean=True,preprocessing__with_std=True,ridge__alpha=10).fit(X,Y)\n",
    "\n",
    "prediction = pipe.predict(X)\n",
    "\n",
    "pipe.score(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ridge Regression\n",
      "train_loss : 0.58335643944\n"
     ]
    }
   ],
   "source": [
    "test_loss_list = []\n",
    "train_loss_list = []\n",
    "#Ridge Regression\n",
    "print \"Starting ridge Regression\"\n",
    "#for i in range(-10,10):\n",
    "threshold = 10000\n",
    "alpha = 0.00000001;\n",
    "clf = Ridge(alpha=alpha)\n",
    "clf.fit(X_train, y_train)\n",
    "ytestpredict = clf.predict(X_test)\n",
    "ytrainpredict = clf.predict(X_train)\n",
    "# print y_test.shape\n",
    "# print ytestpredict.shape\n",
    "# print y_train.shape\n",
    "# print ytrainpredict.shape\n",
    "#test_loss = compute_square_loss(ytestpredict,y_test,threshold)\n",
    "train_loss = compute_square_loss(ytrainpredict,y_train,threshold)\n",
    "#test_loss_list.append(test_loss)\n",
    "#train_loss_list.append(train_loss)\n",
    "print \"train_loss :\" , train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_normalization(train, test):\n",
    "    \"\"\"Rescale the data so that each feature in the training set is in\n",
    "    the interval [0,1], and apply the same transformations to the test\n",
    "    set, using the statistics computed on the training set.\n",
    "    Args:\n",
    "        train - training set, a 2D numpy array of size (num_instances, num_features)\n",
    "        test  - test set, a 2D numpy array of size (num_instances, num_features)\n",
    "    Returns:\n",
    "        train_normalized - training set after normalization\n",
    "        test_normalized  - test set after normalization\n",
    "    \"\"\"\n",
    "    #print \"inside feature_normalization\"\n",
    "    #print train\n",
    "    #print test\n",
    "    nptrain = np.array(train, dtype=np.float64)\n",
    "    nptest = np.array(test, dtype=np.float64)\n",
    "    #print nptrain\n",
    "    r,c = nptrain.shape\n",
    "    #print r\n",
    "    #print c\n",
    "    maxvalues = []\n",
    "    minvalues = []\n",
    "    for i in range(0,c,1):\n",
    "        #print \"%d\" %(i)\n",
    "        vslice = nptrain[:,i:i+1]\n",
    "        #print vslice.shape\n",
    "        vslicemax = vslice.max()\n",
    "        vslicemin = vslice.min()\n",
    "        #print \"vslicemax is %d\" %(vslicemax)\n",
    "        #print \"vslicemin is %d\" %(vslicemin)\n",
    "        maxvalues.append(vslicemax-vslicemin)\n",
    "        minvalues.append(vslicemin)\n",
    "\n",
    "    #print \"Outside for loop\"\n",
    "    npmax = np.array(maxvalues)\n",
    "    npmin = np.array(minvalues)\n",
    "\n",
    "\n",
    "    rows= len(npmax)\n",
    "    # print \"inside normalisation\"\n",
    "    # print rows\n",
    "    for i in range(0,rows):\n",
    "        if (npmax[i] - npmin[i] == 0):\n",
    "            npmax[i] = npmin[i] + 1\n",
    "\n",
    "\n",
    "    train_normalized =  np.array((nptrain - npmin) / (npmax - npmin), dtype=np.float64)\n",
    "    #print train_normalized\n",
    "    test_normalized =  np.array((nptest - npmin) / (npmax - npmin), dtype=np.float64)\n",
    "    #print test_normalized\n",
    "    return train_normalized,test_normalized\n",
    "\n",
    "\n",
    "def compute_square_loss(y1, y2, threshold):\n",
    "    r,c = y1.shape\n",
    "    J = y1 - y2\n",
    "    loss = np.divide(J,y2)\n",
    "    meanLoss = np.sum(np.square(loss))/r\n",
    "    return meanLoss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ridge Regression\n",
      "test_loss : 0.804499625793   train_loss : 0.583356438483\n",
      "test_loss : 0.804499733815   train_loss : 0.583356429766\n",
      "test_loss : 0.804499611794   train_loss : 0.583356342599\n",
      "test_loss : 0.804498259836   train_loss : 0.583355470975\n",
      "test_loss : 0.804484733716   train_loss : 0.583346760071\n",
      "test_loss : 0.804350225247   train_loss : 0.583260181244\n",
      "test_loss : 0.803077139363   train_loss : 0.582444944988\n",
      "test_loss : 0.795183252894   train_loss : 0.5776424936\n",
      "test_loss : 0.78806764021   train_loss : 0.57568124265\n",
      "test_loss : 0.785682293743   train_loss : 0.578323832407\n",
      "test_loss : 0.77186867874   train_loss : 0.577975216301\n",
      "test_loss : 0.742889145366   train_loss : 0.580315118563\n",
      "test_loss : 0.719786204417   train_loss : 0.605873380806\n",
      "test_loss : 0.755010803196   train_loss : 0.678996156912\n",
      "test_loss : 0.83973432827   train_loss : 0.779389621928\n",
      "test_loss : 0.880917459544   train_loss : 0.820430452048\n",
      "test_loss : 0.886779049288   train_loss : 0.826118732459\n",
      "test_loss : 0.88739009712   train_loss : 0.826710074621\n",
      "test_loss : 0.887451461362   train_loss : 0.826769443442\n",
      "test_loss : 0.887457600392   train_loss : 0.82677538268\n",
      "Starting Lasso Regression\n"
     ]
    }
   ],
   "source": [
    "inputFrame = pd.read_csv(\"DesignMatrix\")\n",
    "# print inputFrame.tail()\n",
    "input = inputFrame.as_matrix()\n",
    "X = input[:,1:-1]\n",
    "Y = input[:,-1:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size =500, random_state=5)\n",
    "\n",
    "Xnan = np.isnan(X)\n",
    "np.isnan(Y)\n",
    "\n",
    "r,c = X.shape\n",
    "for i in range(0,r):\n",
    "    for j in range(0,c):\n",
    "        if Xnan[i,j] == True:\n",
    "            print i,j\n",
    "\n",
    "X_train, X_test =  feature_normalization(X_train, X_test)\n",
    "\n",
    "Xnan = np.isnan(X_train)\n",
    "r,c = X_train.shape\n",
    "for i in range(0,r):\n",
    "    for j in range(0,c):\n",
    "        if Xnan[i,j] == True:\n",
    "            print i,j\n",
    "\n",
    "Xnan = np.isnan(X_test)\n",
    "r,c = X_test.shape\n",
    "for i in range(0,r):\n",
    "    for j in range(0,c):\n",
    "        if Xnan[i,j] == True:\n",
    "            print i,j\n",
    "\n",
    "\n",
    "test_loss_list = []\n",
    "train_loss_list = []\n",
    "#Ridge Regression\n",
    "print \"Starting ridge Regression\"\n",
    "for i in range(-10,10):\n",
    "    threshold = 10000\n",
    "    alpha = 10**i;\n",
    "    clf = Ridge(alpha=alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    ytestpredict = clf.predict(X_test)\n",
    "    ytrainpredict = clf.predict(X_train)\n",
    "    # print y_test.shape\n",
    "    # print ytestpredict.shape\n",
    "    # print y_train.shape\n",
    "    # print ytrainpredict.shape\n",
    "    test_loss = compute_square_loss(ytestpredict,y_test,threshold)\n",
    "    train_loss = compute_square_loss(ytrainpredict,y_train,threshold)\n",
    "    test_loss_list.append(test_loss)\n",
    "    train_loss_list.append(train_loss)\n",
    "    print \"test_loss :\", test_loss , \"  train_loss :\" , train_loss\n",
    "    #print i, compute_square_loss(ytestpredict,y_test,threshold), compute_square_loss(ytrainpredict,y_train,threshold)\n",
    "\n",
    "#Lasso Regression\n",
    "print \"Starting Lasso Regression\"\n",
    "for i in range(-10,10):\n",
    "    threshold = 10000\n",
    "    alpha = 10**i;\n",
    "    clf = linear_model.Lasso(alpha=alpha)\n",
    "    clf.fit(X_train,y_train)\n",
    "    ytestpredict = clf.predict(X_test)\n",
    "    ytrainpredict = clf.predict(X_train)\n",
    "    ytestpredict = np.reshape(ytestpredict, (len(ytestpredict), 1))\n",
    "    ytrainpredict = np.reshape(ytrainpredict, (len(ytrainpredict), 1))\n",
    "    # print y_test.shape\n",
    "    # print ytestpredict.shape\n",
    "    # print y_train.shape\n",
    "    # print ytrainpredict.shape\n",
    "    #print i, compute_square_loss(y_test,ytestpredict,threshold), compute_square_loss(y_train,ytrainpredict,threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
